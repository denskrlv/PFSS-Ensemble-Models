{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Feature Subset Selection with Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:30:44.542293Z",
     "start_time": "2025-01-16T11:30:43.994107Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import compute_feature_frequency, load_uci_dataset, train_ensemble_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCI Breast Cancer Clinical Records Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:30:44.547015Z",
     "start_time": "2025-01-16T11:30:44.545339Z"
    }
   },
   "outputs": [],
   "source": [
    "FILE_PATH = \"../data/hcvdat0.csv\" # Change this to the path of the dataset\n",
    "REPO_ID = 571 # Change this to the repository ID of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:30:44.583432Z",
     "start_time": "2025-01-16T11:30:44.581427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset from UCI Machine Learning Repository if it does not exist\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    clinical_records_metadata, clinical_records = load_uci_dataset(repo_id=REPO_ID)\n",
    "    print(clinical_records_metadata)\n",
    "    clinical_records.to_csv(FILE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:30:44.595696Z",
     "start_time": "2025-01-16T11:30:44.587790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 615\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Category</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ALB</th>\n",
       "      <th>ALP</th>\n",
       "      <th>ALT</th>\n",
       "      <th>AST</th>\n",
       "      <th>BIL</th>\n",
       "      <th>CHE</th>\n",
       "      <th>CHOL</th>\n",
       "      <th>CREA</th>\n",
       "      <th>GGT</th>\n",
       "      <th>PROT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0=Blood Donor</td>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>38.5</td>\n",
       "      <td>52.5</td>\n",
       "      <td>7.7</td>\n",
       "      <td>22.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6.93</td>\n",
       "      <td>3.23</td>\n",
       "      <td>106.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0=Blood Donor</td>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>38.5</td>\n",
       "      <td>70.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>11.17</td>\n",
       "      <td>4.80</td>\n",
       "      <td>74.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>76.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0=Blood Donor</td>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>46.9</td>\n",
       "      <td>74.7</td>\n",
       "      <td>36.2</td>\n",
       "      <td>52.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.84</td>\n",
       "      <td>5.20</td>\n",
       "      <td>86.0</td>\n",
       "      <td>33.2</td>\n",
       "      <td>79.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0=Blood Donor</td>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>43.2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30.6</td>\n",
       "      <td>22.6</td>\n",
       "      <td>18.9</td>\n",
       "      <td>7.33</td>\n",
       "      <td>4.74</td>\n",
       "      <td>80.0</td>\n",
       "      <td>33.8</td>\n",
       "      <td>75.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0=Blood Donor</td>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>39.2</td>\n",
       "      <td>74.1</td>\n",
       "      <td>32.6</td>\n",
       "      <td>24.8</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.15</td>\n",
       "      <td>4.32</td>\n",
       "      <td>76.0</td>\n",
       "      <td>29.9</td>\n",
       "      <td>68.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       Category  Age Sex   ALB   ALP   ALT   AST   BIL    CHE  \\\n",
       "0           1  0=Blood Donor   32   m  38.5  52.5   7.7  22.1   7.5   6.93   \n",
       "1           2  0=Blood Donor   32   m  38.5  70.3  18.0  24.7   3.9  11.17   \n",
       "2           3  0=Blood Donor   32   m  46.9  74.7  36.2  52.6   6.1   8.84   \n",
       "3           4  0=Blood Donor   32   m  43.2  52.0  30.6  22.6  18.9   7.33   \n",
       "4           5  0=Blood Donor   32   m  39.2  74.1  32.6  24.8   9.6   9.15   \n",
       "\n",
       "   CHOL   CREA   GGT  PROT  \n",
       "0  3.23  106.0  12.1  69.0  \n",
       "1  4.80   74.0  15.6  76.5  \n",
       "2  5.20   86.0  33.2  79.3  \n",
       "3  4.74   80.0  33.8  75.7  \n",
       "4  4.32   76.0  29.9  68.7  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from CSV file\n",
    "clinical_records = pd.read_csv(FILE_PATH)\n",
    "print(\"Number of samples:\", len(clinical_records))\n",
    "clinical_records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:30:44.622071Z",
     "start_time": "2025-01-16T11:30:44.619674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 589\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values by dropping rows with any missing values\n",
    "clinical_records = clinical_records.dropna()\n",
    "print(\"Number of samples:\", len(clinical_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target after dropping missing values\n",
    "X = clinical_records.drop(columns=[\"Unnamed: 0\", \"Category\"])  # Drop the first column and target\n",
    "\n",
    "# map the gender column to integers m->1, f->0\n",
    "X['Sex'] = X['Sex'].map({'m': 1, 'f': 0})\n",
    "\n",
    "#select the category column as the target\n",
    "y = clinical_records[\"Category\"]\n",
    "#select only the first character of the string for each value in y and convert it to an integer\n",
    "y = y.apply(lambda x: int(x[0]))\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Multiple Ensembles on All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:30:49.613783Z",
     "start_time": "2025-01-16T11:30:44.659104Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Models with GridSearch:   0%|          | 0/5 [00:00<?, ?it/s]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  20%|██        | 1/5 [00:06<00:27,  6.95s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  40%|████      | 2/5 [00:11<00:16,  5.38s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  60%|██████    | 3/5 [00:15<00:09,  4.88s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  80%|████████  | 4/5 [00:19<00:04,  4.65s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier\n",
      "AdaBoost               0.960452\n",
      "Gradient Boosting      0.926554\n",
      "LDA                    0.966102\n",
      "Logistic Regression    0.949153\n",
      "Random Forest          0.954802\n",
      "SVM                    0.960452\n",
      "Name: Accuracy, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "ensemble_results = train_ensemble_models(X_train, X_test, y_train, y_test, n_ensembles=5, n_features_sample=5, random_state=42, verbose=False) # no probabilities were inserted\n",
    "\n",
    "results_df = pd.DataFrame(ensemble_results)\n",
    "results_default = results_df.groupby(\"Classifier\")[\"Accuracy\"].mean()\n",
    "print(results_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Multiple Ensembles on Probabilistic Features (Mutual Information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:30:49.658741Z",
     "start_time": "2025-01-16T11:30:49.644546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Probabilities (Mutual Information):\n",
      "[0.         0.         0.12080712 0.11571771 0.12601803 0.19307922\n",
      " 0.10198185 0.12161827 0.0783143  0.07500287 0.04969066 0.01776997]\n"
     ]
    }
   ],
   "source": [
    "mutual_info = mutual_info_classif(X_train, y_train)\n",
    "probabilities = mutual_info / np.sum(mutual_info)  # Normalize to create a probability distribution\n",
    "\n",
    "print(\"Feature Probabilities (Mutual Information):\")\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:30:53.563196Z",
     "start_time": "2025-01-16T11:30:49.681055Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Models with GridSearch:  20%|██        | 1/5 [00:03<00:14,  3.51s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  40%|████      | 2/5 [00:07<00:11,  3.74s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  60%|██████    | 3/5 [00:10<00:07,  3.62s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  80%|████████  | 4/5 [00:14<00:03,  3.66s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier\n",
      "AdaBoost               0.944633\n",
      "Gradient Boosting      0.938983\n",
      "LDA                    0.943503\n",
      "Logistic Regression    0.943503\n",
      "Random Forest          0.946893\n",
      "SVM                    0.940113\n",
      "Name: Accuracy, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "ensemble_results = train_ensemble_models(X_train, X_test, y_train, y_test, probabilities, n_ensembles=5, n_features_sample=5, random_state=42, verbose=False)\n",
    "\n",
    "results_df = pd.DataFrame(ensemble_results)\n",
    "results_mutual_information = results_df.groupby(\"Classifier\")[\"Accuracy\"].mean()\n",
    "print(results_mutual_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:30:53.588503Z",
     "start_time": "2025-01-16T11:30:53.585564Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4gMeiCwsvlfh",
    "outputId": "75390383-eab7-49b9-98e0-b168a6cc86fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Selection Frequency:\n",
      "[0.  0.  0.8 0.4 0.2 0.8 0.4 0.6 0.4 0.6 0.4 0.4]\n"
     ]
    }
   ],
   "source": [
    "# Analyze uncertainty in feature importance\n",
    "feature_frequency = compute_feature_frequency(ensemble_results, X_train.shape[1])\n",
    "\n",
    "print(\"\\nFeature Selection Frequency:\")\n",
    "print(feature_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Multiple Ensembles on Probabilistic Features (Interaction Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:30:53.680378Z",
     "start_time": "2025-01-16T11:30:53.620409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Probabilities (Interaction Scores):\n",
      "[0.03907333 0.00498679 0.05015069 0.13591432 0.14373398 0.21013785\n",
      " 0.06284569 0.15013282 0.04185235 0.04683836 0.07288722 0.0414466 ]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Calculate pairwise interaction scores\n",
    "interaction_matrix = np.zeros((X_train.shape[1], X_train.shape[1]))\n",
    "for tree in rf.estimators_:\n",
    "    for feature_idx, importance in enumerate(tree.feature_importances_):\n",
    "        interaction_matrix[feature_idx] += importance\n",
    "\n",
    "interaction_scores = interaction_matrix.sum(axis=1) / rf.n_estimators\n",
    "probabilities = interaction_scores / interaction_scores.sum()\n",
    "\n",
    "print(\"Feature Probabilities (Interaction Scores):\")\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:30:57.638490Z",
     "start_time": "2025-01-16T11:30:53.692987Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Models with GridSearch:  20%|██        | 1/5 [00:03<00:13,  3.37s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  40%|████      | 2/5 [00:06<00:10,  3.52s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  60%|██████    | 3/5 [00:10<00:07,  3.58s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  80%|████████  | 4/5 [00:14<00:03,  3.61s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier\n",
      "AdaBoost               0.941243\n",
      "Gradient Boosting      0.929944\n",
      "LDA                    0.938983\n",
      "Logistic Regression    0.952542\n",
      "Random Forest          0.951412\n",
      "SVM                    0.938983\n",
      "Name: Accuracy, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Train Multiple Ensembles on Probabilistically Sampled Subsets\n",
    "ensemble_results = train_ensemble_models(X_train, X_test, y_train, y_test, probabilities, n_ensembles=5, n_features_sample=5, random_state=42, verbose=False)\n",
    "\n",
    "results_df = pd.DataFrame(ensemble_results)\n",
    "results_interaction_score = results_df.groupby(\"Classifier\")[\"Accuracy\"].mean()\n",
    "print(results_interaction_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:30:57.671852Z",
     "start_time": "2025-01-16T11:30:57.669483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Selection Frequency:\n",
      "[0.4 0.  0.6 0.8 0.4 0.6 0.4 0.8 0.4 0.4 0.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "# Analyze uncertainty in feature importance\n",
    "feature_frequency = compute_feature_frequency(ensemble_results, X_train.shape[1])\n",
    "\n",
    "print(\"\\nFeature Selection Frequency:\")\n",
    "print(feature_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Multiple Ensembles on Probabilistic Features (Linear Correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:30:57.698353Z",
     "start_time": "2025-01-16T11:30:57.696847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Probabilities (Linear Correlation):\n",
      "Age     0.044778\n",
      "Sex     0.010117\n",
      "ALB     0.089236\n",
      "ALP     0.010838\n",
      "ALT     0.056558\n",
      "AST     0.195632\n",
      "BIL     0.167794\n",
      "CHE     0.116255\n",
      "CHOL    0.099848\n",
      "CREA    0.076071\n",
      "GGT     0.127368\n",
      "PROT    0.005505\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "linear_correlation = np.abs(X_train.corrwith(y_train))\n",
    "probabilities_correlation = linear_correlation / linear_correlation.sum()\n",
    "\n",
    "print(\"Feature Probabilities (Linear Correlation):\")\n",
    "print(probabilities_correlation)\n",
    "\n",
    "probabilities = probabilities_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Models with GridSearch:   0%|          | 0/5 [00:00<?, ?it/s]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  20%|██        | 1/5 [00:03<00:14,  3.55s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  40%|████      | 2/5 [00:06<00:10,  3.49s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  60%|██████    | 3/5 [00:10<00:07,  3.61s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier\n",
      "AdaBoost               0.932203\n",
      "Gradient Boosting      0.934463\n",
      "LDA                    0.938983\n",
      "Logistic Regression    0.942373\n",
      "Random Forest          0.942373\n",
      "SVM                    0.934463\n",
      "Name: Accuracy, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "ensemble_results = train_ensemble_models(X_train, X_test, y_train, y_test, probabilities, n_ensembles=5, n_features_sample=5, random_state=42, verbose=False)\n",
    "\n",
    "results_df = pd.DataFrame(ensemble_results)\n",
    "results_linear_correlation = results_df.groupby(\"Classifier\")[\"Accuracy\"].mean()\n",
    "print(results_linear_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Selection Frequency:\n",
      "[0.2 0.2 1.  0.2 0.2 0.8 0.6 0.8 0.4 0.2 0.4 0. ]\n"
     ]
    }
   ],
   "source": [
    "# Analyze uncertainty in feature importance\n",
    "feature_frequency = compute_feature_frequency(ensemble_results, X_train.shape[1])\n",
    "\n",
    "print(\"\\nFeature Selection Frequency:\")\n",
    "print(feature_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Multiple Ensembles on Probabilistic Features (Indormation Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:30:57.728639Z",
     "start_time": "2025-01-16T11:30:57.727196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Probabilities (Information Gain):\n",
      "[0.         0.00505105 0.11867885 0.11519388 0.13128368 0.18896891\n",
      " 0.09852386 0.12065471 0.08074242 0.07627976 0.04828186 0.01634104]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "info_gain = mutual_info_classif(X_train, y_train, discrete_features=False)\n",
    "probabilities_info_gain = info_gain / np.sum(info_gain)\n",
    "\n",
    "print(\"Feature Probabilities (Information Gain):\")\n",
    "print(probabilities_info_gain)\n",
    "\n",
    "probabilities = probabilities_info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Models with GridSearch:   0%|          | 0/5 [00:00<?, ?it/s]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  60%|██████    | 3/5 [00:10<00:07,  3.61s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier\n",
      "AdaBoost               0.941243\n",
      "Gradient Boosting      0.936723\n",
      "LDA                    0.941243\n",
      "Logistic Regression    0.946893\n",
      "Random Forest          0.946893\n",
      "SVM                    0.948023\n",
      "Name: Accuracy, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "ensemble_results = train_ensemble_models(X_train, X_test, y_train, y_test, probabilities, n_ensembles=5, n_features_sample=5, random_state=42, verbose=False)\n",
    "\n",
    "results_df = pd.DataFrame(ensemble_results)\n",
    "results_information_gain = results_df.groupby(\"Classifier\")[\"Accuracy\"].mean()\n",
    "print(results_information_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Selection Frequency:\n",
      "[0.  0.2 0.8 0.4 0.8 1.  0.6 0.4 0.2 0.4 0.  0.2]\n"
     ]
    }
   ],
   "source": [
    "# Analyze uncertainty in feature importance\n",
    "feature_frequency = compute_feature_frequency(ensemble_results, X_train.shape[1])\n",
    "\n",
    "print(\"\\nFeature Selection Frequency:\")\n",
    "print(feature_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Multiple Ensembles on Probabilistic Features (Spearman Rank Correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Probabilities (Spearman Rank Correlation):\n",
      "Age     0.034428\n",
      "Sex     0.020163\n",
      "ALB     0.056143\n",
      "ALP     0.076337\n",
      "ALT     0.122039\n",
      "AST     0.177107\n",
      "BIL     0.129505\n",
      "CHE     0.077133\n",
      "CHOL    0.111665\n",
      "CREA    0.042203\n",
      "GGT     0.134201\n",
      "PROT    0.019076\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "spearman_correlation = np.abs(X_train.apply(lambda x: spearmanr(x, y_train)[0]))\n",
    "probabilities_spearman = spearman_correlation / spearman_correlation.sum()\n",
    "\n",
    "print(\"Feature Probabilities (Spearman Rank Correlation):\")\n",
    "print(probabilities_spearman)\n",
    "\n",
    "probabilities = probabilities_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Models with GridSearch:   0%|          | 0/5 [00:00<?, ?it/s]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  20%|██        | 1/5 [00:03<00:15,  3.92s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  40%|████      | 2/5 [00:07<00:11,  3.97s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier\n",
      "AdaBoost               0.942373\n",
      "Gradient Boosting      0.933333\n",
      "LDA                    0.937853\n",
      "Logistic Regression    0.945763\n",
      "Random Forest          0.940113\n",
      "SVM                    0.935593\n",
      "Name: Accuracy, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "ensemble_results = train_ensemble_models(X_train, X_test, y_train, y_test, probabilities, n_ensembles=5, n_features_sample=5, random_state=42, verbose=False)\n",
    "\n",
    "results_df = pd.DataFrame(ensemble_results)\n",
    "results_spearman_correlation = results_df.groupby(\"Classifier\")[\"Accuracy\"].mean()\n",
    "print(results_spearman_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Selection Frequency:\n",
      "[0.2 0.2 0.2 0.4 0.2 0.8 0.6 0.8 0.4 0.  0.6 0.6]\n"
     ]
    }
   ],
   "source": [
    "# Analyze uncertainty in feature importance\n",
    "feature_frequency = compute_feature_frequency(ensemble_results, X_train.shape[1])\n",
    "\n",
    "print(\"\\nFeature Selection Frequency:\")\n",
    "print(feature_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Multiple Ensembles on Probabilistic Features (Shannon Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Probabilities (Shannon Entropy):\n",
      "[0.06360281 0.01192012 0.08713173 0.10177804 0.09669875 0.09462856\n",
      " 0.08652191 0.10089442 0.09589797 0.07511366 0.09760885 0.08820319]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "# Calculate Shannon entropy for each feature\n",
    "def calculate_shannon_entropy(feature):\n",
    "    # Compute the probability distribution of unique values\n",
    "    values, counts = np.unique(feature, return_counts=True)\n",
    "    probabilities = counts / np.sum(counts)\n",
    "    return entropy(probabilities, base=2)  # Base 2 for Shannon entropy\n",
    "\n",
    "# Check if X_train is a DataFrame and adjust accordingly\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    shannon_entropies = [calculate_shannon_entropy(X_train.iloc[:, i]) for i in range(X_train.shape[1])]\n",
    "else:\n",
    "    shannon_entropies = [calculate_shannon_entropy(X_train[:, i]) for i in range(X_train.shape[1])]\n",
    "\n",
    "\n",
    "\n",
    "probabilities_entropy = shannon_entropies / np.sum(shannon_entropies)\n",
    "probabilities = probabilities_entropy\n",
    "\n",
    "print(\"Feature Probabilities (Shannon Entropy):\")\n",
    "print(probabilities_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Models with GridSearch:   0%|          | 0/5 [00:00<?, ?it/s]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  20%|██        | 1/5 [00:03<00:15,  3.87s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  40%|████      | 2/5 [00:07<00:11,  3.86s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  80%|████████  | 4/5 [00:15<00:03,  3.73s/it]c:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier\n",
      "AdaBoost               0.940113\n",
      "Gradient Boosting      0.928814\n",
      "LDA                    0.932203\n",
      "Logistic Regression    0.937853\n",
      "Random Forest          0.941243\n",
      "SVM                    0.924294\n",
      "Name: Accuracy, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "ensemble_results = train_ensemble_models(X_train, X_test, y_train, y_test, probabilities, n_ensembles=5, n_features_sample=5, random_state=42, verbose=False)\n",
    "\n",
    "results_df = pd.DataFrame(ensemble_results)\n",
    "results_shannon_entropy = results_df.groupby(\"Classifier\")[\"Accuracy\"].mean()\n",
    "print(results_shannon_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Selection Frequency:\n",
      "[0.6 0.  0.4 0.4 0.2 0.4 0.4 0.6 0.8 0.4 0.4 0.4]\n"
     ]
    }
   ],
   "source": [
    "# Analyze uncertainty in feature importance\n",
    "feature_frequency = compute_feature_frequency(ensemble_results, X_train.shape[1])\n",
    "\n",
    "print(\"\\nFeature Selection Frequency:\")\n",
    "print(feature_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group all the results for each similarity measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Results Table:\n",
      "                      Default  Mutual Information  Interaction Scores  \\\n",
      "AdaBoost             0.960452            0.944633            0.941243   \n",
      "Gradient Boosting    0.926554            0.938983            0.929944   \n",
      "LDA                  0.966102            0.943503            0.938983   \n",
      "Logistic Regression  0.949153            0.943503            0.952542   \n",
      "Random Forest        0.954802            0.946893            0.951412   \n",
      "SVM                  0.960452            0.940113            0.938983   \n",
      "\n",
      "                     Linear Correlation  Information Gain  \\\n",
      "AdaBoost                       0.932203          0.941243   \n",
      "Gradient Boosting              0.934463          0.936723   \n",
      "LDA                            0.938983          0.941243   \n",
      "Logistic Regression            0.942373          0.946893   \n",
      "Random Forest                  0.942373          0.946893   \n",
      "SVM                            0.934463          0.948023   \n",
      "\n",
      "                     Spearman Rank Correlation  Shannon Entropy  \n",
      "AdaBoost                              0.942373         0.940113  \n",
      "Gradient Boosting                     0.933333         0.928814  \n",
      "LDA                                   0.937853         0.932203  \n",
      "Logistic Regression                   0.945763         0.937853  \n",
      "Random Forest                         0.940113         0.941243  \n",
      "SVM                                   0.935593         0.924294  \n"
     ]
    }
   ],
   "source": [
    "#%pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine measurement results into a table and plot\n",
    "measurements_results = {\n",
    "    \"Default\": results_default,\n",
    "    \"Mutual Information\": results_mutual_information,\n",
    "    \"Interaction Scores\": results_interaction_score,\n",
    "    \"Linear Correlation\": results_linear_correlation,\n",
    "    \"Information Gain\": results_information_gain,\n",
    "    \"Spearman Rank Correlation\": results_spearman_correlation,\n",
    "    \"Shannon Entropy\": results_shannon_entropy,\n",
    "}\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "for name, result in measurements_results.items():\n",
    "    result_df = result.rename(name)  # Rename series for clarity\n",
    "    combined_df = pd.concat([combined_df, result_df], axis=1)\n",
    "\n",
    "print(\"Combined Results Table:\")\n",
    "print(combined_df)\n",
    "\n",
    "\n",
    "# # Convert each result series to a DataFrame and merge them\n",
    "# combined_df = pd.DataFrame()\n",
    "# for name, result in measurements_results.items():\n",
    "#     result_df = result.rename(name)  # Rename series for clarity\n",
    "#     combined_df = pd.concat([combined_df, result_df], axis=1)\n",
    "\n",
    "# print(\"Combined Results Table:\")\n",
    "# print(combined_df)\n",
    "\n",
    "# # Plot the table\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# ax.axis('tight')\n",
    "# ax.axis('off')\n",
    "# table = ax.table(cellText=combined_df.round(3).values,\n",
    "#                   colLabels=combined_df.columns,\n",
    "#                   rowLabels=combined_df.index,\n",
    "#                   loc='center',\n",
    "#                   cellLoc='center')\n",
    "# plt.title(\"Measurement Results Table\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Improvements Table:\n",
      "                     Mutual Information  Interaction Scores  \\\n",
      "AdaBoost                      -0.015819           -0.019209   \n",
      "Gradient Boosting              0.012429            0.003390   \n",
      "LDA                           -0.022599           -0.027119   \n",
      "Logistic Regression           -0.005650            0.003390   \n",
      "Random Forest                 -0.007910           -0.003390   \n",
      "SVM                           -0.020339           -0.021469   \n",
      "\n",
      "                     Linear Correlation  Information Gain  \\\n",
      "AdaBoost                      -0.028249         -0.019209   \n",
      "Gradient Boosting              0.007910          0.010169   \n",
      "LDA                           -0.027119         -0.024859   \n",
      "Logistic Regression           -0.006780         -0.002260   \n",
      "Random Forest                 -0.012429         -0.007910   \n",
      "SVM                           -0.025989         -0.012429   \n",
      "\n",
      "                     Spearman Rank Correlation  Shannon Entropy  \n",
      "AdaBoost                             -0.018079        -0.020339  \n",
      "Gradient Boosting                     0.006780         0.002260  \n",
      "LDA                                  -0.028249        -0.033898  \n",
      "Logistic Regression                  -0.003390        -0.011299  \n",
      "Random Forest                        -0.014689        -0.013559  \n",
      "SVM                                  -0.024859        -0.036158  \n"
     ]
    }
   ],
   "source": [
    "measurements_improvements = {\n",
    "    \"Mutual Information\": results_mutual_information - results_default,\n",
    "    \"Interaction Scores\": results_interaction_score - results_default,\n",
    "    \"Linear Correlation\": results_linear_correlation - results_default,\n",
    "    \"Information Gain\": results_information_gain - results_default,\n",
    "    \"Spearman Rank Correlation\": results_spearman_correlation - results_default,\n",
    "    \"Shannon Entropy\": results_shannon_entropy - results_default,\n",
    "}\n",
    "\n",
    "combined_improvements_df = pd.DataFrame()\n",
    "for name, result in measurements_improvements.items():\n",
    "    result_df_impr = result.rename(name)  # Rename series for clarity\n",
    "    combined_improvements_df = pd.concat([combined_improvements_df, result_df_impr], axis=1)\n",
    "\n",
    "print(\"Combined Improvements Table:\")\n",
    "print(combined_improvements_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
