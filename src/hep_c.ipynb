{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Probabilistic Feature Subset Selection with Ensemble Models"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:15:34.259606Z",
     "start_time": "2025-01-15T15:15:33.187791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.utils import compute_feature_frequency, load_uci_dataset, train_ensemble_models"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## UCI Breast Cancer Clinical Records Dataset"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:18:24.785730Z",
     "start_time": "2025-01-15T15:18:24.780947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Load dataset from UCI Machine Learning Repository if it does not exist\n",
    "# from ucimlrepo import fetch_ucirepo\n",
    "#\n",
    "# # fetch dataset\n",
    "# hcv_data = fetch_ucirepo(id=571)\n",
    "#\n",
    "# # data (as pandas dataframes)\n",
    "# X = hcv_data.data.features\n",
    "# y = hcv_data.data.targets\n",
    "#\n",
    "# # metadata\n",
    "# print(hcv_data.metadata)\n",
    "#\n",
    "# # variable information\n",
    "# print(hcv_data.variables)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:24:33.771142Z",
     "start_time": "2025-01-15T15:24:33.748296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataset from CSV file\n",
    "FILE_PATH = \"../data/hcvdat0.csv\"\n",
    "\n",
    "hep_c_records = pd.read_csv(FILE_PATH)\n",
    "print(\"Number of samples:\", len(hep_c_records))\n",
    "hep_c_records.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0       Category  Age Sex   ALB   ALP   ALT   AST   BIL    CHE  \\\n",
       "0           1  0=Blood Donor   32   m  38.5  52.5   7.7  22.1   7.5   6.93   \n",
       "1           2  0=Blood Donor   32   m  38.5  70.3  18.0  24.7   3.9  11.17   \n",
       "2           3  0=Blood Donor   32   m  46.9  74.7  36.2  52.6   6.1   8.84   \n",
       "3           4  0=Blood Donor   32   m  43.2  52.0  30.6  22.6  18.9   7.33   \n",
       "4           5  0=Blood Donor   32   m  39.2  74.1  32.6  24.8   9.6   9.15   \n",
       "\n",
       "   CHOL   CREA   GGT  PROT  \n",
       "0  3.23  106.0  12.1  69.0  \n",
       "1  4.80   74.0  15.6  76.5  \n",
       "2  5.20   86.0  33.2  79.3  \n",
       "3  4.74   80.0  33.8  75.7  \n",
       "4  4.32   76.0  29.9  68.7  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Category</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ALB</th>\n",
       "      <th>ALP</th>\n",
       "      <th>ALT</th>\n",
       "      <th>AST</th>\n",
       "      <th>BIL</th>\n",
       "      <th>CHE</th>\n",
       "      <th>CHOL</th>\n",
       "      <th>CREA</th>\n",
       "      <th>GGT</th>\n",
       "      <th>PROT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0=Blood Donor</td>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>38.5</td>\n",
       "      <td>52.5</td>\n",
       "      <td>7.7</td>\n",
       "      <td>22.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6.93</td>\n",
       "      <td>3.23</td>\n",
       "      <td>106.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0=Blood Donor</td>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>38.5</td>\n",
       "      <td>70.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>11.17</td>\n",
       "      <td>4.80</td>\n",
       "      <td>74.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>76.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0=Blood Donor</td>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>46.9</td>\n",
       "      <td>74.7</td>\n",
       "      <td>36.2</td>\n",
       "      <td>52.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.84</td>\n",
       "      <td>5.20</td>\n",
       "      <td>86.0</td>\n",
       "      <td>33.2</td>\n",
       "      <td>79.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0=Blood Donor</td>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>43.2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30.6</td>\n",
       "      <td>22.6</td>\n",
       "      <td>18.9</td>\n",
       "      <td>7.33</td>\n",
       "      <td>4.74</td>\n",
       "      <td>80.0</td>\n",
       "      <td>33.8</td>\n",
       "      <td>75.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0=Blood Donor</td>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>39.2</td>\n",
       "      <td>74.1</td>\n",
       "      <td>32.6</td>\n",
       "      <td>24.8</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.15</td>\n",
       "      <td>4.32</td>\n",
       "      <td>76.0</td>\n",
       "      <td>29.9</td>\n",
       "      <td>68.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocess Dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:24:55.065175Z",
     "start_time": "2025-01-15T15:24:55.055090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Handle missing values by dropping rows with any missing values\n",
    "hep_c_records = hep_c_records.dropna()\n",
    "print(\"Number of samples:\", len(hep_c_records))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 589\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:41:27.836935Z",
     "start_time": "2025-01-15T15:41:27.824727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Separate features and target after dropping missing values\n",
    "X = hep_c_records.drop(columns=[\"Unnamed: 0\", \"Category\"])  # Drop the first column and target\n",
    "\n",
    "# map the gender column to integers m->1, f->0\n",
    "X['Sex'] = X['Sex'].map({'m': 1, 'f': 0})\n",
    "\n",
    "#select the category column as the target\n",
    "y = hep_c_records[\"Category\"]\n",
    "#select only the first character of the string for each value in y and convert it to an integer\n",
    "y = y.apply(lambda x: int(x[0]))\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "      ..\n",
      "608    0\n",
      "609    0\n",
      "610    0\n",
      "611    0\n",
      "612    0\n",
      "Name: Sex, Length: 589, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Assign Probabilities to Features using Mutual Information\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:46:02.231854Z",
     "start_time": "2025-01-15T15:46:02.178490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mutual_info = mutual_info_classif(X_train, y_train)\n",
    "probabilities_mi = mutual_info / np.sum(mutual_info)  # Normalize to create a probability distribution\n",
    "\n",
    "print(\"Feature Probabilities (Mutual Information):\")\n",
    "print(probabilities_mi)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Probabilities (Mutual Information):\n",
      "[0.00594148 0.01085856 0.12249352 0.11484162 0.13240261 0.18738324\n",
      " 0.09841207 0.1156652  0.07711062 0.07893319 0.04422028 0.01173761]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Assign Probabilities to Features using Linear Correlation\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T16:00:50.670085Z",
     "start_time": "2025-01-15T16:00:50.661175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "linear_correlation = np.abs(X_train.corrwith(y_train))\n",
    "probabilities_correlation = linear_correlation / linear_correlation.sum()\n",
    "\n",
    "print(\"Feature Probabilities (Linear Correlation):\")\n",
    "print(probabilities_correlation)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Assign Probabilities to Features using Coherence"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "res_coherence ="
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Assign Probabilities to Features using Recurrence Rate"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Assign Probabilities to Features using Symbolic Entropy"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train Multiple Ensembles on Probabilistically Sampled Subsets\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "probabilities = probabilities_mi"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:46:28.813399Z",
     "start_time": "2025-01-15T15:46:07.776565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ensemble_results = train_ensemble_models(X_train, X_test, y_train, y_test, probabilities, n_ensembles=5, n_features_sample=5, random_state=42, verbose=False)\n",
    "\n",
    "results_df = pd.DataFrame(ensemble_results)\n",
    "results_df.groupby(\"Classifier\")[\"Accuracy\"].mean()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Models with GridSearch:   0%|          | 0/5 [00:00<?, ?it/s]C:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  20%|██        | 1/5 [00:06<00:27,  6.84s/it]C:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  40%|████      | 2/5 [00:10<00:14,  4.90s/it]C:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  60%|██████    | 3/5 [00:13<00:08,  4.31s/it]C:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  80%|████████  | 4/5 [00:17<00:04,  4.01s/it]C:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "                                                                              \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Classifier\n",
       "AdaBoost               0.943503\n",
       "Gradient Boosting      0.933333\n",
       "LDA                    0.937853\n",
       "Logistic Regression    0.952542\n",
       "Random Forest          0.943503\n",
       "SVM                    0.946893\n",
       "Name: Accuracy, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "# Analyze uncertainty in feature importance\n",
    "feature_frequency = compute_feature_frequency(ensemble_results, X_train.shape[1])\n",
    "\n",
    "print(\"\\nFeature Selection Frequency:\")\n",
    "print(feature_frequency)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4gMeiCwsvlfh",
    "outputId": "75390383-eab7-49b9-98e0-b168a6cc86fc",
    "ExecuteTime": {
     "end_time": "2025-01-15T15:49:43.967123Z",
     "start_time": "2025-01-15T15:49:43.957406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Selection Frequency:\n",
      "[0.2 0.  0.2 0.6 0.4 0.8 0.6 0.6 0.8 0.6 0.2 0. ]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Assign probabilities to features using interaction scores"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:49:48.270319Z",
     "start_time": "2025-01-15T15:49:48.137482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Calculate pairwise interaction scores\n",
    "interaction_matrix = np.zeros((X_train.shape[1], X_train.shape[1]))\n",
    "for tree in rf.estimators_:\n",
    "    for feature_idx, importance in enumerate(tree.feature_importances_):\n",
    "        interaction_matrix[feature_idx] += importance\n",
    "\n",
    "interaction_scores = interaction_matrix.sum(axis=1) / rf.n_estimators\n",
    "probabilities = interaction_scores / interaction_scores.sum()\n",
    "\n",
    "print(\"Feature Probabilities (Interaction Scores):\")\n",
    "print(probabilities)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Probabilities (Interaction Scores):\n",
      "[0.03907333 0.00498679 0.05015069 0.13591432 0.14373398 0.21013785\n",
      " 0.06284569 0.15013282 0.04185235 0.04683836 0.07288722 0.0414466 ]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train Multiple Ensembles on Probabilistically Sampled Subsets\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:50:12.146633Z",
     "start_time": "2025-01-15T15:49:54.614474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ensemble_results = train_ensemble_models(X_train, X_test, y_train, y_test, probabilities, n_ensembles=5, n_features_sample=5, random_state=42, verbose=False)\n",
    "\n",
    "results_df = pd.DataFrame(ensemble_results)\n",
    "results_df.groupby(\"Classifier\")[\"Accuracy\"].mean()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Models with GridSearch:  60%|██████    | 3/5 [00:10<00:07,  3.51s/it]C:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "Ensemble Models with GridSearch:  80%|████████  | 4/5 [00:13<00:03,  3.50s/it]C:\\GithubProjects\\PFSS-Ensemble-Models\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "                                                                              \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Classifier\n",
       "AdaBoost               0.944633\n",
       "Gradient Boosting      0.935593\n",
       "LDA                    0.946893\n",
       "Logistic Regression    0.957062\n",
       "Random Forest          0.952542\n",
       "SVM                    0.950282\n",
       "Name: Accuracy, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analyze uncertainty in feature importance"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:57:07.211619Z",
     "start_time": "2025-01-15T11:55:29.950475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Analyze uncertainty in feature importance\n",
    "feature_frequency = compute_feature_frequency(ensemble_results, X_train.shape[1])\n",
    "\n",
    "print(\"\\nFeature Selection Frequency:\")\n",
    "print(feature_frequency)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Selection Frequency:\n",
      "[0.2 1.  1.  0.2 0.6 1.  0.4 0.4 0.2]\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ]
}
